{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import psutil\n",
    "import pynvml\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('/home/meribejayson/Desktop/Projects/SharkCNN/training_models/dataloaders/')\n",
    "\n",
    "from test_dataset import SharkDatasetTest as SharkDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_path = \"/home/meribejayson/Desktop/Projects/SharkCNN/training_models/LOGISTIC-REG/train-final-3/lr_weights_train_3.tar\"\n",
    "ann_model_path = \"/home/meribejayson/Desktop/Projects/SharkCNN/training_models/ANN/train-1/ann_weights_train_1.tar\"\n",
    "\n",
    "state_dict_lr = torch.load(lr_model_path)\n",
    "state_dict_ann = torch.load(ann_model_path)\n",
    "\n",
    "torch.manual_seed(12)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"Couldn't find CUDA\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "\n",
    "image_width = 1920\n",
    "image_height = 1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ANN(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_size):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         hidden_layer_num = int(input_size / 2)\n",
    "\n",
    "#         self.linear1 = nn.Linear(input_size, hidden_layer_num)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.linear2 = nn.Linear(hidden_layer_num, hidden_layer_num)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.linear3 = nn.Linear(hidden_layer_num + input_size, 1)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.linear1(x)\n",
    "#         h = self.relu1(h)\n",
    "#         h = self.linear2(h)\n",
    "#         h = self.relu2(h)\n",
    "#         h = torch.cat((x, h), dim=1)\n",
    "#         h = self.linear3(h)\n",
    "        \n",
    "#         return self.sig(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_layer_num = int(input_size / 2)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_layer_num)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_layer_num + input_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear1(x)\n",
    "        h = self.relu1(h)\n",
    "        h = torch.cat((x, h), dim=1)\n",
    "        h = self.linear2(h)\n",
    "        \n",
    "        return self.sig(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegresion(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return self.sig(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SharkDataset()\n",
    "\n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=500_000, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegresion(\n",
       "  (linear): Linear(in_features=85, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_state = torch.load(\"/home/meribejayson/Desktop/Projects/SharkCNN/training_models/ANN/train-4/ann_weights_train_4.tar\")\n",
    "lr_state = torch.load(\"/home/meribejayson/Desktop/Projects/SharkCNN/training_models/LOGISTIC-REG/train-final-6/lr_weights_train_6.tar\")\n",
    "\n",
    "ann_model = ANN(85)\n",
    "ann_model.load_state_dict(ann_state)\n",
    "ann_model.to(device)\n",
    "\n",
    "\n",
    "lr_model = LogisticRegresion(85)\n",
    "lr_model.load_state_dict(lr_state)\n",
    "lr_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, data_labels):\n",
    "    conf_thresholds = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] \n",
    "    precision_varying = []\n",
    "    recall_varying = []\n",
    "    f1_score_varying = []\n",
    "\n",
    "    for thresh in conf_thresholds:\n",
    "        TP = preds[(preds > thresh) & (data_labels == 1.0)].size(dim=0)\n",
    "        FP = preds[(preds > thresh) & (data_labels == 0.0)].size(dim=0)\n",
    "        FN = preds[(preds < thresh) & (data_labels == 1.0)].size(dim=0)\n",
    "\n",
    "        if(TP + FN + FP == 0):\n",
    "            return None\n",
    "\n",
    "        curr_thresh_precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "        curr_thresh_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "\n",
    "        curr_thresh_f1_score = (2 * curr_thresh_precision * curr_thresh_recall) / (curr_thresh_recall + curr_thresh_precision) if (curr_thresh_recall + curr_thresh_precision) != 0 else 0\n",
    "\n",
    "\n",
    "        precision_varying.append(curr_thresh_precision)\n",
    "        recall_varying.append(curr_thresh_recall)\n",
    "        f1_score_varying.append(curr_thresh_f1_score)\n",
    "\n",
    "    MAP_50 = precision_varying[0]\n",
    "    MAP50_95 = np.mean(np.array(precision_varying))\n",
    "\n",
    "    best_f1_score_idx = np.argmax(np.array(curr_thresh_f1_score))\n",
    "    \n",
    "    f1_score = f1_score_varying[best_f1_score_idx]\n",
    "    recall = recall_varying[best_f1_score_idx]\n",
    "    precision = precision_varying[best_f1_score_idx]\n",
    "\n",
    "    return {\n",
    "        \"MAP50\": MAP_50,\n",
    "        \"MAP50_95\": MAP50_95,\n",
    "        \"F1_Score\": f1_score,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader):\n",
    "    ann_model.eval()\n",
    "    lr_model.eval()\n",
    "\n",
    "    map50_ann = []\n",
    "    map50_95_ann = []\n",
    "    f1_score_ann = []\n",
    "    recall_ann = []\n",
    "    precision_ann = []\n",
    "\n",
    "    map50_lr = []\n",
    "    map50_95_lr = []\n",
    "    f1_score_lr = []\n",
    "    recall_lr = []\n",
    "    precision_lr = []\n",
    "\n",
    "    curr_iter = 1\n",
    "\n",
    "    while(True):\n",
    "        with torch.no_grad():\n",
    "            for point in data_loader:\n",
    "                data_inputs = point[:, :-1].to(device).float()\n",
    "                data_labels = point[:, -1].to(device).float()\n",
    "\n",
    "                ann_preds = ann_model(data_inputs)\n",
    "                ann_preds = ann_preds.squeeze(dim=1)\n",
    "\n",
    "                lr_preds = lr_model(data_inputs)\n",
    "                lr_preds = lr_preds.squeeze(dim=1)\n",
    "\n",
    "                metrics_ann_dict = calculate_metrics(ann_preds, data_labels)\n",
    "                metrics_lr_dict = calculate_metrics(lr_preds, data_labels)\n",
    "\n",
    "            \n",
    "                if metrics_ann_dict == None or metrics_lr_dict == None:\n",
    "                    print(f\"Skipped iteration {curr_iter}\")\n",
    "                    continue\n",
    "            \n",
    "                map50_ann.append(metrics_ann_dict[\"MAP50\"])\n",
    "                map50_95_ann.append(metrics_ann_dict[\"MAP50_95\"])\n",
    "                f1_score_ann.append(metrics_ann_dict[\"F1_Score\"])\n",
    "                recall_ann.append(metrics_ann_dict[\"Recall\"])\n",
    "                precision_ann.append(metrics_ann_dict[\"Precision\"])\n",
    "\n",
    "                map50_lr.append(metrics_lr_dict[\"MAP50\"])\n",
    "                map50_95_lr.append(metrics_lr_dict[\"MAP50_95\"])\n",
    "                f1_score_lr.append(metrics_lr_dict[\"F1_Score\"])\n",
    "                recall_lr.append(metrics_lr_dict[\"Recall\"])\n",
    "                precision_lr.append(metrics_lr_dict[\"Precision\"])\n",
    "\n",
    "                MAP_50_ANN = np.mean(np.array(map50_ann))\n",
    "                MAP_50_95_ANN = np.mean(np.array(map50_95_ann))\n",
    "                f1_score_ANN = np.mean(np.array(f1_score_ann))\n",
    "                recall_ANN = np.mean(np.array(recall_ann))\n",
    "                precision_ANN = np.mean(np.array(precision_ann))\n",
    "\n",
    "                MAP_50_LR = np.mean(np.array(map50_lr))\n",
    "                MAP_50_95_LR = np.mean(np.array(map50_95_lr))\n",
    "                f1_score_LR = np.mean(np.array(f1_score_lr))\n",
    "                recall_LR = np.mean(np.array(recall_lr))\n",
    "                precision_LR = np.mean(np.array(precision_lr))\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                print(f'Current iteration: {curr_iter}')\n",
    "                print('='*30)\n",
    "                print(\"ANN Metrics\")\n",
    "                print(f\"MAP50: {MAP_50_ANN} MAP50-95: {MAP_50_95_ANN} Precision: {precision_ANN} Recall: {recall_ANN} F1 score: {f1_score_ANN}\")\n",
    "                print(\"\\n\")\n",
    "                print(\"LR Metrics\")\n",
    "                print(f\"MAP50: {MAP_50_LR} MAP50-95: {MAP_50_95_LR} Precision: {precision_LR} Recall: {recall_LR} F1 score: {f1_score_LR}\")\n",
    "                print('='*30)\n",
    "                print(f\"CPU Usage: {psutil.cpu_percent()}% GPU memory usage: {int(info.used / info.total)}% \\n\")\n",
    "                \n",
    "        curr_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     22\u001b[0m             data_inputs \u001b[38;5;241m=\u001b[39m point[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     23\u001b[0m             data_labels \u001b[38;5;241m=\u001b[39m point[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/cs229proj/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_model(test_data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
