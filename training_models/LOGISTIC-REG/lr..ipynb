{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import psutil\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"Couldn't find CUDA\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "megaset_path = \"/home/meribejayson/Desktop/Projects/SharkCNN/datasets-reduced/megaset/\"\n",
    "megaset_train_images_path = \"/home/meribejayson/Desktop/Projects/SharkCNN/datasets-reduced/megaset/train/images/\"\n",
    "megaset_train_labels_path = \"/home/meribejayson/Desktop/Projects/SharkCNN/datasets-reduced/megaset/train/labels/\"\n",
    "\n",
    "image_width = 1920\n",
    "image_height = 1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegresion(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return self.sig(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharkDataset(data.IterableDataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SharkDataset).__init__()\n",
    "        self.image_names = os.listdir(megaset_train_images_path)\n",
    "        self.num_images = len(self.image_names)\n",
    "        self.curr_image_ordering = np.arange(self.num_images, dtype=np.int32)\n",
    "        self.curr_image_idx = 0\n",
    "\n",
    "        self.reset_random_image_ord()\n",
    "\n",
    "        self.transform = A.Compose(\n",
    "            [\n",
    "               A.Blur(p=0.01, blur_limit=(3,7)),\n",
    "               A.MedianBlur(p=0.01, blur_limit=(3,7)),\n",
    "               A.ToGray(p=0.01),\n",
    "               A.CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def reset_random_image_ord(self):\n",
    "        self.curr_image_order = np.random.shuffle(self.curr_image_ordering)\n",
    "        self.curr_image_idx = 0\n",
    "\n",
    "    \"\"\"\n",
    "        The following return a numpy array representing the image in BGR format\n",
    "    \"\"\"\n",
    "    def get_random_image(self):\n",
    "        if(self.curr_image_idx == self.num_images):\n",
    "            self.reset_random_image_order()\n",
    "\n",
    "        image_name = self.image_names[self.curr_image_ordering[self.curr_image_idx]]\n",
    "        file_path = megaset_train_images_path + image_name\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        self.curr_image_idx += 1\n",
    "\n",
    "        return (image_name, image)\n",
    "\n",
    "    def transform_image(self, image):\n",
    "        return self.transform(image=image)[\"image\"]\n",
    "    \n",
    "    \"\"\"\n",
    "        Assumes input image is in grayscale\n",
    "    \"\"\"\n",
    "    def get_canny_output(self, image):\n",
    "        return cv2.Canny(image, 50, 100)\n",
    "    \n",
    "    \"\"\"\n",
    "        Expecting image in BGR format\n",
    "    \"\"\"\n",
    "    def get_color_gradients(self, image):\n",
    "        blue_dy = ski.filters.sobel_h(image[:, :, 0])\n",
    "        blue_dx = ski.filters.sobel_v(image[:, :, 0])\n",
    "\n",
    "        green_dy = ski.filters.sobel_h(image[:, :, 1])\n",
    "        green_dx = ski.filters.sobel_v(image[:, :, 1])\n",
    "\n",
    "        red_dy = ski.filters.sobel_h(image[:, :, 2])\n",
    "        red_dx = ski.filters.sobel_v(image[:, :, 2])\n",
    "\n",
    "        return (blue_dx, blue_dy, green_dx, green_dy, red_dx, red_dy)\n",
    "    \n",
    "    def get_direction(self, image_dx, image_dy):\n",
    "        peturbation = 2e-125\n",
    "\n",
    "        image_dy_peturb = copy.deepcopy(image_dy)\n",
    "        image_dx_peturb = copy.deepcopy(image_dx)\n",
    "\n",
    "        image_dy_peturb[image_dy == 0.0] = peturbation\n",
    "        image_dx_peturb[image_dx == 0.0] = peturbation\n",
    "\n",
    "        return np.arctan(image_dy_peturb / image_dx_peturb)\n",
    "\n",
    "    \"\"\"\n",
    "        Expecting image in BGR format\n",
    "\n",
    "        return gabor feature image per channel (8, 1080, 1920)\n",
    "    \"\"\"\n",
    "    def get_gabor_feature_vector(self, image):\n",
    "        size = 20\n",
    "        pi_2 = np.pi / 2\n",
    "\n",
    "        sigma = np.array([1.0, 1.0, 3.0, 1.0, 3.0, 1.0, 2.0, 1.0])\n",
    "        theta = np.array([0.0, 0.0, 0.2617993877991494, 0.5235987755982988, 0.2617993877991494, 0.2617993877991494, 0.0, 0.0])\n",
    "        lambd = np.array([10.0, 15.0, 20.0, 15.0, 20.0, 20.0, 15.0, 20.0])\n",
    "        gamma = np.array([0.5, 1.5, 1.5, 1.5, 1.0, 1.5, 1.5, 1.0])\n",
    "        psi = np.array([pi_2, pi_2, pi_2, pi_2, pi_2, pi_2, pi_2, pi_2])\n",
    "\n",
    "        R = []\n",
    "        G = []\n",
    "        B = []\n",
    "\n",
    "        for idx in range(sigma.shape[0]): \n",
    "            kern = cv2.getGaborKernel((size, size), sigma[idx], theta[idx], lambd[idx], gamma[idx], psi[idx], ktype=cv2.CV_32F)\n",
    "            B.append(cv2.filter2D(image[:, :, 0], -1, kern))\n",
    "            G.append(cv2.filter2D(image[:, :, 1], -1, kern))\n",
    "            R.append(cv2.filter2D(image[:, :, 2], -1, kern))\n",
    "            \n",
    "        \n",
    "        R_np = np.array(R)\n",
    "        G_np = np.array(G)\n",
    "        B_np = np.array(B)\n",
    "         \n",
    "        return (B_np, G_np, R_np)\n",
    "    \n",
    "    def get_gabor_feature_gradients(self, B_np, G_np, R_np):\n",
    "        R_dx = []\n",
    "        G_dx = []\n",
    "        B_dx = []\n",
    "\n",
    "        R_dy = []\n",
    "        G_dy = []\n",
    "        B_dy = []\n",
    "\n",
    "        for i in range(B_np.shape[0]):\n",
    "            B_dx.append(ski.filters.sobel_v(B_np[i, :, :]))\n",
    "            B_dy.append(ski.filters.sobel_h(B_np[i, :, :]))\n",
    "\n",
    "            G_dx.append(ski.filters.sobel_v(G_np[i, :, :]))\n",
    "            G_dy.append(ski.filters.sobel_h(G_np[i, :, :]))\n",
    "\n",
    "            R_dx.append(ski.filters.sobel_v(R_np[i, :, :]))\n",
    "            R_dy.append(ski.filters.sobel_h(R_np[i, :, :]))\n",
    "        \n",
    "        R_dx_np = np.array(R_dx)\n",
    "        R_dy_np = np.array(R_dy)\n",
    "\n",
    "        G_dx_np = np.array(G_dx)\n",
    "        G_dy_np = np.array(G_dy)\n",
    "\n",
    "        B_dx_np = np.array(B_dx)\n",
    "        B_dy_np = np.array(B_dy)\n",
    "\n",
    "        return (B_dx_np, B_dy_np, G_dx_np, G_dy_np, R_dx_np, R_dy_np) \n",
    "\n",
    "    def is_pixel_in_box(self, image_name, pixel_loc_x, pixel_loc_y):\n",
    "        label_name = image_name.split(\".\")[0] + \".txt\"\n",
    "        label_path = megaset_train_labels_path + label_name\n",
    "\n",
    "        labels = []\n",
    "        \n",
    "        # I assume images that don't have an sharks\n",
    "        try:\n",
    "            with open(label_path, \"r\") as labels_doc:\n",
    "                labels = labels_doc.read().splitlines()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Could not find {label_path}\") \n",
    "            return False  \n",
    "\n",
    "        \n",
    "        for labels_string in labels:\n",
    "            box_keypoints = [float(keypoint_string) for keypoint_string in labels_string.split(\" \")]\n",
    "            points = np.array([[box_keypoints[1], box_keypoints[2]], \n",
    "                               [box_keypoints[3], box_keypoints[4]], \n",
    "                               [box_keypoints[5], box_keypoints[6]], \n",
    "                               [box_keypoints[7], box_keypoints[8]]], dtype=np.float32)\n",
    "            \n",
    "            pixel_norm_loc = np.array([pixel_loc_x / image_width, pixel_loc_y / image_height])\n",
    "\n",
    "            if(cv2.pointPolygonTest(points,  pixel_norm_loc, False) >= 0):\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "    def test_shark_dataset_functions(self):\n",
    "        # Test get_random_image\n",
    "        image_name, image = self.get_random_image()\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Test transform_image\n",
    "        transformed_image = self.transform_image(image)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Transformed Image\")\n",
    "        plt.imshow(cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        \n",
    "        # Test get_canny_output (Assuming the transformed image is suitable)\n",
    "        canny_output = self.get_canny_output(cv2.cvtColor(transformed_image, cv2.COLOR_RGB2GRAY))\n",
    "        plt.figure()\n",
    "        plt.title(\"Canny Output\")\n",
    "        plt.imshow(canny_output, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        # Test get_color_gradients\n",
    "        blue_dx, blue_dy, green_dx, green_dy, red_dx, red_dy = self.get_color_gradients(image)\n",
    "        # Visualizing one of the gradients\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title(\"Gradient - Blue Channel DX\")\n",
    "        plt.imshow(blue_dx, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        # No direct visualization for get_direction as it's more of a computation method\n",
    "        \n",
    "        # Test get_gabor_feature_vector\n",
    "        B_np, G_np, R_np = self.get_gabor_feature_vector(image)\n",
    "        # Visualize one of the Gabor feature images\n",
    "        plt.figure()\n",
    "        plt.title(\"Gabor Feature - Blue Channel\")\n",
    "        plt.imshow(B_np[0], cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        # Test get_gabor_feature_gradients (Using Gabor features from previous step)\n",
    "        B_dx_np, B_dy_np, G_dx_np, G_dy_np, R_dx_np, R_dy_np = self.get_gabor_feature_gradients(B_np, G_np, R_np)\n",
    "        # Visualize one of the gradient images\n",
    "        plt.figure()\n",
    "        plt.title(\"Gabor Feature Gradient - Blue Channel DX\")\n",
    "        plt.imshow(B_dx_np[0], cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        # Testing is_pixel_in_box function\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        overlay = image.copy()\n",
    "        \n",
    "        # Iterate over each pixel in the image\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                if self.is_pixel_in_box(image_name, x, y):\n",
    "                    overlay[y, x] = [255, 0, 0]  # Red for inside box\n",
    "\n",
    "        # Blend the original image with the overlay\n",
    "        alpha = 0.25  # Transparency factor\n",
    "        output_image = cv2.addWeighted(image_rgb, 1 - alpha, cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB), alpha, 0)\n",
    "\n",
    "        # Display the original and the output image\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.title('Original Image')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(output_image)\n",
    "        plt.title('Overlay Image')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # I  standardize each image because each image has a different feature distribution, which will make it hard for our model to learn weights that generalize across all images\n",
    "    def standardize(self, mat):\n",
    "        return(mat - np.mean(mat)) / np.std(mat)\n",
    "    \n",
    "    def generate_image_features(self):\n",
    "        image_name, image = self.get_random_image()\n",
    "\n",
    "        image = self.transform_image(image)\n",
    "\n",
    "        # Canny\n",
    "        image_canny_stand = self.standardize(self.get_canny_output(image))\n",
    "\n",
    "        # Color Gradient\n",
    "        blue_dx, blue_dy, green_dx, green_dy, red_dx, red_dy = self.get_color_gradients(image)\n",
    "        \n",
    "        blue_dx_stand = self.standardize(blue_dx)\n",
    "        blue_dy_stand = self.standardize(blue_dy)\n",
    "        green_dx_stand = self.standardize(green_dx)\n",
    "        green_dy_stand = self.standardize(green_dy)\n",
    "        red_dx_stand = self.standardize(red_dx)\n",
    "        red_dy_stand =  self.standardize(red_dy)\n",
    "\n",
    "        # Gradient Direction\n",
    "        blue_direction = self.get_direction(blue_dx, blue_dy)\n",
    "        green_direction = self.get_direction(green_dx, green_dy)\n",
    "        red_direction = self.get_direction(red_dx, red_dy)\n",
    "\n",
    "\n",
    "        # Texture Feature Vectors and Gradients\n",
    "        blue_text_stand, green_text_stand, red_text_stand = self.get_gabor_feature_vector(image)\n",
    "\n",
    "        blue_text_dx_stand, blue_text_dy_stand, green_text_dx_stand, green_text_dy_stand, red_text_dx_stand, red_text_dy_stand = self.get_gabor_feature_gradients(blue_text_stand, green_text_stand, red_text_stand)\n",
    "\n",
    "        for i in range(blue_text_stand.shape[0]):\n",
    "            blue_text_stand[i, :, :] = self.standardize(blue_text_stand[i, :, :])\n",
    "            green_text_stand[i, :, :] = self.standardize(green_text_stand[i, :, :])\n",
    "            red_text_stand[i, :, :] = self.standardize(red_text_stand[i, :, :])\n",
    "\n",
    "        for i in range(blue_text_dx_stand.shape[0]):\n",
    "            blue_text_dx_stand[i, :, :] = self.standardize(blue_text_dx_stand[i, :, :])\n",
    "            blue_text_dy_stand[i, :, :] = self.standardize(blue_text_dy_stand[i, :, :])\n",
    "\n",
    "            green_text_dx_stand[i, :, :] = self.standardize(green_text_dx_stand[i, :, :])\n",
    "            green_text_dy_stand[i, :, :] = self.standardize(green_text_dy_stand[i, :, :])\n",
    "\n",
    "            red_text_dx_stand[i, :, :] = self.standardize(red_text_dx_stand[i, :, :])\n",
    "            red_text_dy_stand[i, :, :] = self.standardize(red_text_dy_stand[i, :, :])\n",
    "\n",
    "        image_per_pixel_feats = np.empty((image_height * image_width, 86))\n",
    "        \n",
    "        # Color\n",
    "        blue_stand = self.standardize(image[:, :, 0])\n",
    "        green_stand = self.standardize(image[:, :, 1])\n",
    "        red_stand = self.standardize(image[:, :, 2])\n",
    "\n",
    "        # Creating features\n",
    "\n",
    "        curr_idx = 0\n",
    "\n",
    "        for y in range(image_height):\n",
    "            for x in range(image_width):\n",
    "                curr_y = 1 if self.is_pixel_in_box(image_name, x, y) else 0 \n",
    "\n",
    "                pixel_features = np.array([red_stand[y, x], green_stand[y, x], blue_stand[y, x], \n",
    "                                           image_canny_stand[y, x], \n",
    "                                           red_dx_stand[y, x], red_dy_stand[y, x], green_dx_stand[y, x], green_dy_stand[y, x], blue_dx_stand[y, x], blue_dy_stand[y, x], \n",
    "                                           red_direction[y, x], green_direction[y, x], blue_direction[y, x], \n",
    "                                           red_text_stand[0, y, x], red_text_stand[1, y, x], red_text_stand[2, y, x], red_text_stand[3, y, x], red_text_stand[4, y, x], red_text_stand[5, y, x], red_text_stand[6, y, x], red_text_stand[7, y, x],\n",
    "                                           blue_text_stand[0, y, x], blue_text_stand[1, y, x], blue_text_stand[2, y, x], blue_text_stand[3, y, x], blue_text_stand[4, y, x], blue_text_stand[5, y, x], blue_text_stand[6, y, x], blue_text_stand[7, y, x],\n",
    "                                           green_text_stand[0, y, x], green_text_stand[1, y, x], green_text_stand[2, y, x], green_text_stand[3, y, x], green_text_stand[4, y, x], green_text_stand[5, y, x], green_text_stand[6, y, x], green_text_stand[7, y, x],\n",
    "                                           red_text_dx_stand[0, y, x], red_text_dx_stand[1, y, x], red_text_dx_stand[2, y, x], red_text_dx_stand[3, y, x], red_text_dx_stand[4, y, x], red_text_dx_stand[5, y, x], red_text_dx_stand[6, y, x], red_text_dx_stand[7, y, x],\n",
    "                                           blue_text_dx_stand[0, y, x], blue_text_dx_stand[1, y, x], blue_text_dx_stand[2, y, x], blue_text_dx_stand[3, y, x], blue_text_dx_stand[4, y, x], blue_text_dx_stand[5, y, x], blue_text_dx_stand[6, y, x], blue_text_dx_stand[7, y, x],\n",
    "                                           green_text_dx_stand[0, y, x], green_text_dx_stand[1, y, x], green_text_dx_stand[2, y, x], green_text_dx_stand[3, y, x], green_text_dx_stand[4, y, x], green_text_dx_stand[5, y, x], green_text_dx_stand[6, y, x], green_text_dx_stand[7, y, x],\n",
    "                                           red_text_dy_stand[0, y, x], red_text_dy_stand[1, y, x], red_text_dy_stand[2, y, x], red_text_dy_stand[3, y, x], red_text_dy_stand[4, y, x], red_text_dy_stand[5, y, x], red_text_dy_stand[6, y, x], red_text_dy_stand[7, y, x],\n",
    "                                           blue_text_dy_stand[0, y, x], blue_text_dy_stand[1, y, x], blue_text_dy_stand[2, y, x], blue_text_dy_stand[3, y, x], blue_text_dy_stand[4, y, x], blue_text_dy_stand[5, y, x], blue_text_dy_stand[6, y, x], blue_text_dy_stand[7, y, x],\n",
    "                                           green_text_dy_stand[0, y, x], green_text_dy_stand[1, y, x], green_text_dy_stand[2, y, x], green_text_dy_stand[3, y, x], green_text_dy_stand[4, y, x], green_text_dy_stand[5, y, x], green_text_dy_stand[6, y, x], green_text_dy_stand[7, y, x],\n",
    "                                           curr_y])\n",
    "                \n",
    "                image_per_pixel_feats[curr_idx, :] = pixel_features\n",
    "\n",
    "                curr_idx += 1\n",
    "        \n",
    "        return image_per_pixel_feats\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = np.empty((image_height * image_width * 2, 86))\n",
    "        data[:(image_height * image_width), :] = self.generate_image_features()\n",
    "        data[(image_height * image_width):(image_height * image_width * 2), :] = self.generate_image_features()\n",
    "        \n",
    "        np.random.shuffle(data)\n",
    "         \n",
    "        return iter(torch.from_numpy(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_dataset = SharkDataset()\n",
    "data_loader = data.DataLoader(shark_dataset, batch_size=1_000_000, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"./train-final-2/lr_weights_train_2.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegresion(85)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_NUM = 2e120\n",
    "target_loss_change = 1e-5\n",
    "exps_in_iter = (image_height * image_width * 2)\n",
    "kappa = 1 / 323\n",
    "kappa_inv = 323\n",
    "coef = (1 + kappa) / 2\n",
    "small_peturb = 2e-120\n",
    "\n",
    "def train_model(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    last_average_loss = LARGE_NUM\n",
    "    curr_average_loss = 0\n",
    "    curr_iter = 1\n",
    "\n",
    "    while(np.abs(curr_average_loss - last_average_loss) > target_loss_change):\n",
    "        \n",
    "        total_iter_avg_loss = 0\n",
    "\n",
    "        for point in data_loader:\n",
    "            data_inputs = point[:, :-1].to(device).float()\n",
    "            data_labels = point[:, -1].to(device).float()\n",
    "\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=1)\n",
    "\n",
    "            weights = torch.clone(data_labels)\n",
    "            weights[data_labels == 0.0] = 1\n",
    "            weights[data_labels == 1.0] = kappa_inv\n",
    "\n",
    "            weights = coef * weights\n",
    "            \n",
    "            data_labels[data_labels == 0.0] = data_labels[data_labels == 0.0] + small_peturb\n",
    "            data_labels[data_labels == 1.0] = data_labels[data_labels == 1.0] - small_peturb\n",
    "            \n",
    "            loss_module = nn.BCELoss(weight=weights)\n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_iter_avg_loss += loss.item()\n",
    " \n",
    "        last_average_loss = curr_average_loss\n",
    "        curr_average_loss = total_iter_avg_loss\n",
    "        \n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        clear_output(wait=True)\n",
    "        print(f'Current iteration: {curr_iter - 1}, Average Loss: {last_average_loss}')\n",
    "        print(f'Current iteration: {curr_iter}, Average Loss: {curr_average_loss}')\n",
    "        print(f\"CPU Usage: {psutil.cpu_percent()}% GPU memory usage: {int(info.used / info.total)}% \\n\")\n",
    "\n",
    "        print(\"Current Parameters:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.data)\n",
    "\n",
    "        curr_iter += 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 392, Average Loss: 0.02220639423467219\n",
      "Current iteration: 393, Average Loss: 0.022215599776245654\n",
      "CPU Usage: 17.0% GPU memory usage: 0% \n",
      "\n",
      "Current Parameters:\n",
      "linear.weight tensor([[ 8.8438e-02, -4.1447e-02,  5.7988e-02,  2.5508e-01,  2.1914e-01,\n",
      "          4.1002e-02,  1.2102e-01,  2.4286e-02,  2.7373e-01,  5.5927e-02,\n",
      "          1.5209e-02, -7.9003e-02, -1.0960e-02, -1.7376e-01, -2.4090e-01,\n",
      "         -1.8512e-01, -2.1791e-01, -2.7587e-01, -1.2228e-01, -1.0727e-01,\n",
      "         -1.2169e-01, -1.4348e-01, -1.3062e-01, -1.9141e-01, -2.1120e-01,\n",
      "         -2.9247e-01, -1.2279e-01, -1.0668e-01, -1.2639e-01, -1.8587e-01,\n",
      "         -2.6121e-01, -1.7965e-01, -2.2002e-01, -2.6013e-01, -1.2802e-01,\n",
      "         -1.1201e-01, -1.4598e-01,  1.7382e-03,  3.8620e-03, -4.4278e-03,\n",
      "          1.7996e-02, -1.1002e-02,  2.4375e-03,  5.7739e-03, -1.1377e-03,\n",
      "          8.4366e-03,  8.9391e-03,  3.1053e-02,  2.5116e-02,  2.1628e-02,\n",
      "          8.6249e-03,  2.6799e-02,  4.3937e-03, -3.9218e-03, -9.3392e-04,\n",
      "         -3.5854e-02,  1.2282e-02, -4.6147e-02, -1.9868e-03, -1.0342e-02,\n",
      "         -6.3967e-03,  4.6510e-03,  1.6078e-03, -1.2351e-03, -5.8392e-03,\n",
      "          2.4910e-03, -2.2267e-03,  8.2041e-03, -3.0381e-03,  7.0405e-03,\n",
      "          2.2529e-03,  1.3210e-05, -1.4595e-03,  2.0554e-03,  3.4146e-04,\n",
      "          1.1622e-02, -1.9817e-03,  1.7390e-03,  1.5808e-03, -9.3156e-03,\n",
      "         -9.1757e-03, -6.4467e-03, -4.3617e-03,  2.7918e-03, -3.7956e-03]],\n",
      "       device='cuda:0')\n",
      "linear.bias tensor([-6.0242], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[ 8.8438e-02, -4.1447e-02,  5.7988e-02,  2.5508e-01,  2.1914e-01,\n",
      "          4.1002e-02,  1.2102e-01,  2.4286e-02,  2.7373e-01,  5.5927e-02,\n",
      "          1.5209e-02, -7.9003e-02, -1.0960e-02, -1.7376e-01, -2.4090e-01,\n",
      "         -1.8512e-01, -2.1791e-01, -2.7587e-01, -1.2228e-01, -1.0727e-01,\n",
      "         -1.2169e-01, -1.4348e-01, -1.3062e-01, -1.9141e-01, -2.1120e-01,\n",
      "         -2.9247e-01, -1.2279e-01, -1.0668e-01, -1.2639e-01, -1.8587e-01,\n",
      "         -2.6121e-01, -1.7965e-01, -2.2002e-01, -2.6013e-01, -1.2802e-01,\n",
      "         -1.1201e-01, -1.4598e-01,  1.7382e-03,  3.8620e-03, -4.4278e-03,\n",
      "          1.7996e-02, -1.1002e-02,  2.4375e-03,  5.7739e-03, -1.1377e-03,\n",
      "          8.4366e-03,  8.9391e-03,  3.1053e-02,  2.5116e-02,  2.1628e-02,\n",
      "          8.6249e-03,  2.6799e-02,  4.3937e-03, -3.9218e-03, -9.3392e-04,\n",
      "         -3.5854e-02,  1.2282e-02, -4.6147e-02, -1.9868e-03, -1.0342e-02,\n",
      "         -6.3967e-03,  4.6510e-03,  1.6078e-03, -1.2351e-03, -5.8392e-03,\n",
      "          2.4910e-03, -2.2267e-03,  8.2041e-03, -3.0381e-03,  7.0405e-03,\n",
      "          2.2529e-03,  1.3210e-05, -1.4595e-03,  2.0554e-03,  3.4146e-04,\n",
      "          1.1622e-02, -1.9817e-03,  1.7390e-03,  1.5808e-03, -9.3156e-03,\n",
      "         -9.1757e-03, -6.4467e-03, -4.3617e-03,  2.7918e-03, -3.7956e-03]],\n",
      "       device='cuda:0')), ('linear.bias', tensor([-6.0242], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "pynvml.nvmlShutdown()\n",
    "state_dict = model.state_dict()\n",
    "print(state_dict)\n",
    "torch.save(state_dict, \"lr_weights_train_3.tar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
