{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "sys.path.append('/home/meribejayson/Desktop/Projects/SharkCNN/training_models/dataloaders/')\n",
    "\n",
    "megaset_train_images_path = \"/home/meribejayson/Desktop/Projects/SharkCNN/datasets-reduced/megaset/train/images/\"\n",
    "\n",
    "from get_single_image_train_features import SharkImageFeatureGen as FeatureGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_name = \"dji206-1175.jpg\"\n",
    "# image_name = \"dji343-7553.jpg\"\n",
    "image_name = \"dji358-1099.jpg\"\n",
    "\n",
    "feature_gen = FeatureGenerator()\n",
    "features = feature_gen.generate_image_features(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"Couldn't find CUDA\")\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegresion(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return self.sig(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_layer_num = int(input_size / 2)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_layer_num)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_layer_num + input_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear1(x)\n",
    "        h = self.relu1(h)\n",
    "        h = torch.cat((x, h), dim=1)\n",
    "        h = self.linear2(h)\n",
    "        \n",
    "        return self.sig(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegresion(\n",
       "  (linear): Linear(in_features=85, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_state = torch.load(\"/home/meribejayson/Desktop/Projects/SharkCNN/training_models/ANN/train-4/ann_weights_train_4.tar\")\n",
    "lr_state = torch.load(\"/home/meribejayson/Desktop/Projects/SharkCNN/training_models/LOGISTIC-REG/train-final-6/lr_weights_train_6.tar\")\n",
    "\n",
    "ann_model = ANN(85)\n",
    "ann_model.load_state_dict(ann_state)\n",
    "ann_model.to(device)\n",
    "\n",
    "\n",
    "lr_model = LogisticRegresion(85)\n",
    "lr_model.load_state_dict(lr_state)\n",
    "lr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visual/lr_model_visualization.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inputs = features[:, :-1].to(device).float()\n",
    "data_labels = features[:, -1].to(device).float()\n",
    "\n",
    "ann_preds = ann_model(data_inputs)\n",
    "lr_preds = lr_model(data_inputs)\n",
    "\n",
    "dot = make_dot(ann_preds, params=dict(ann_model.named_parameters()))\n",
    "dot.render('./visual/ann_model_visualization', format='png')\n",
    "\n",
    "dot = make_dot(lr_preds, params=dict(lr_model.named_parameters()))\n",
    "dot.render('./visual/lr_model_visualization', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been converted to ONNX format and saved to ann.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Define or load your model\n",
    "model = ANN(85)\n",
    "\n",
    "# Load the model dictionary (Assuming model_dict is your model state dictionary)\n",
    "model_dict_path = '/home/meribejayson/Desktop/Projects/SharkCNN/training_models/ANN/train-4/ann_weights_train_4.tar'\n",
    "model.load_state_dict(torch.load(model_dict_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input that matches the input size the model expects\n",
    "# Example for a model that expects input of size [batch_size, channels, height, width]\n",
    "dummy_input = torch.randn(1, 85)  # Adjust the size as per your model's requirement\n",
    "\n",
    "# Define the path for the ONNX model\n",
    "onnx_model_path = 'ann.onnx'\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  dummy_input,         # model input (or a tuple for multiple inputs)\n",
    "                  onnx_model_path,     # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,  # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,    # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names=['input'],   # the model's input names\n",
    "                  output_names=['output'], # the model's output names\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},  # variable length axes\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "print(f'Model has been converted to ONNX format and saved to {onnx_model_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
